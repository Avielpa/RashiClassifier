#!/usr/bin/env python3
"""
Script to download Sefer HaMitzvot pages from Wikisource and save them in a nested library structure.
Uses the prep file generated by generate_prep_file.py
"""

import json
import requests
import time
import re
from pathlib import Path
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import logging
import concurrent.futures
import threading
from typing import List, Dict, Tuple

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)

class SeferHaMitzvotDownloader:
    def __init__(self, prep_file_path="sefer_hamitzvot_prep.json", recovery_mode=False, max_workers=5):
        self.prep_file_path = prep_file_path
        self.recovery_mode = recovery_mode
        self.max_workers = max_workers
        
        # Create session for each thread
        self.session_factory = lambda: self._create_session()
        
        # Load prep data
        if not Path(prep_file_path).exists():
            raise FileNotFoundError(
                f"Prep file not found: {prep_file_path}\n"
                "Please run 'python generate_prep_file.py' first to create the prep file."
            )
        
        try:
            with open(prep_file_path, 'r', encoding='utf-8') as f:
                self.prep_data = json.load(f)
        except json.JSONDecodeError as e:
            raise ValueError(f"Invalid JSON in prep file {prep_file_path}: {str(e)}")
        except Exception as e:
            raise RuntimeError(f"Error reading prep file {prep_file_path}: {str(e)}")
        
        # Validate prep data structure
        required_keys = ['download_queue', 'structure', 'total_pages']
        missing_keys = [key for key in required_keys if key not in self.prep_data]
        if missing_keys:
            raise ValueError(f"Prep file {prep_file_path} is missing required keys: {missing_keys}")
        
        if not isinstance(self.prep_data['download_queue'], list):
            raise ValueError(f"Prep file {prep_file_path} has invalid 'download_queue' - must be a list")
        
        if len(self.prep_data['download_queue']) == 0:
            raise ValueError(f"Prep file {prep_file_path} has empty download queue")
        
        # Create base directory
        self.base_dir = Path("../content")
        self.base_dir.mkdir(exist_ok=True)
        
        # Statistics with thread safety
        self.downloaded_count = 0
        self.failed_count = 0
        self.failed_urls = []
        self.stats_lock = threading.Lock()
        
        # Progress tracking
        self.total_items = len(self.prep_data['download_queue'])
        self.completed_items = 0
        self.progress_lock = threading.Lock()
    
    def _create_session(self):
        """Create a new session for a thread."""
        session = requests.Session()
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        return session

    def clean_text(self, text):
        """Clean and format the extracted text."""
        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text)
        
        # Remove common Wikisource artifacts
        text = re.sub(r'\[עריכה\]', '', text)
        text = re.sub(r'\[עריכת קוד מקור\]', '', text)
        
        # Remove navigation text and breadcrumbs
        text = re.sub(r'ספר המצוות·[^·]*·>>', '', text)
        text = re.sub(r'עריכה', '', text)
        text = re.sub(r'עריכת קוד מקור', '', text)
        text = re.sub(r'תוכן עניינים', '', text)
        text = re.sub(r'קפיצה לניווט', '', text)
        text = re.sub(r'קפיצה לחיפוש', '', text)
        text = re.sub(r'אוחזר מתוך "[^"]*"', '', text)
        
        # Remove empty lines and excessive whitespace
        text = re.sub(r'\n\s*\n\s*\n', '\n\n', text)
        text = re.sub(r'^\s+|\s+$', '', text, flags=re.MULTILINE)
        
        # Clean up line breaks
        text = re.sub(r'\n\s*\n', '\n\n', text)
        
        # Remove any remaining HTML-like artifacts
        text = re.sub(r'<[^>]+>', '', text)
        
        # Remove any remaining navigation patterns
        text = re.sub(r'\[לשרשים:[^\]]*\]', '', text)
        
        return text.strip()

    def extract_content(self, soup):
        """Extract the main content from the Wikisource page."""
        # Find the main content area
        content_div = soup.find('div', {'id': 'mw-content-text'})
        if not content_div:
            # Try alternative content areas
            content_div = soup.find('div', {'class': 'mw-parser-output'})
            if not content_div:
                # Last resort: try to find any content
                content_div = soup.find('body')
                if not content_div:
                    return None
        
        # Remove navigation elements, edit links, etc.
        for element in content_div.find_all(['script', 'style', 'nav', 'table']):
            element.decompose()
        
        # Remove edit section links and other UI elements
        for element in content_div.find_all(['span', 'div'], class_=['mw-editsection', 'mw-editsection-bracket', 'mw-editsection-divider', 'mw-headline']):
            element.decompose()
        
        # Remove any remaining navigation elements
        for element in content_div.find_all(['a'], href=True):
            if any(href in element.get('href', '') for href in ['#', 'edit', 'action=edit']):
                element.decompose()
        
        # Try to find the main text content
        main_text = []
        
        # First try to find paragraphs with substantial content
        for element in content_div.find_all(['p', 'div']):
            text = element.get_text(strip=True)
            if text and len(text) > 20:  # Only substantial text
                # Skip navigation text
                if any(nav in text for nav in ['עריכה', 'תוכן עניינים', 'קפיצה לניווט', 'קפיצה לחיפוש', 'ספר המצוות·']):
                    continue
                main_text.append(text)
        
        if main_text:
            text = '\n\n'.join(main_text)
        else:
            # Fallback: get all text from the content area
            text = content_div.get_text(separator='\n', strip=True)
            
            # If still no content, try the entire page
            if not text or len(text) < 50:
                text = soup.get_text(separator='\n', strip=True)
        
        return self.clean_text(text)

    def download_page(self, url, filename):
        """Download a single page and save it as text."""
        try:
            # Create session for this thread
            session = self.session_factory()
            
            logging.info(f"Downloading: {filename}")
            
            response = session.get(url, timeout=30)
            response.raise_for_status()
            
            # Parse HTML
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extract content
            content = self.extract_content(soup)
            if not content:
                logging.warning(f"No content extracted from {url}")
                return False
            
            # Save to file (convert .html to .txt)
            txt_filename = filename.replace('.html', '.txt')
            file_path = self.base_dir / txt_filename
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
            
            # Update statistics with thread safety
            with self.stats_lock:
                self.downloaded_count += 1
            
            logging.info(f"Successfully downloaded: {filename}")
            return True
            
        except Exception as e:
            logging.error(f"Failed to download {url}: {str(e)}")
            with self.stats_lock:
                self.failed_count += 1
                self.failed_urls.append((url, filename, str(e)))
            return False

    def create_directory_structure(self):
        """Create the nested directory structure."""
        # Main sections
        sections = {
            'פתיחה': self.base_dir / 'פתיחה',
            'הקדמה': self.base_dir / 'הקדמה', 
            'שרשים': self.base_dir / 'שרשים',
            'מצוות_עשה': self.base_dir / 'מצוות_עשה',
            'מצוות_לא_תעשה': self.base_dir / 'מצוות_לא_תעשה',
            'מקורות': self.base_dir / 'מקורות',
            'קישורים_חיצוניים': self.base_dir / 'קישורים_חיצוניים'
        }
        
        for section_name, section_path in sections.items():
            section_path.mkdir(exist_ok=True)
            logging.info(f"Created directory: {section_path}")
        
        return sections

    def organize_files(self):
        """Organize downloaded files into the nested structure."""
        sections = self.create_directory_structure()
        
        for item in self.prep_data['download_queue']:
            item_type = item['type']
            filename = item['filename']
            txt_filename = filename.replace('.html', '.txt')
            
            # Determine target directory based on type
            if item_type == 'main_section':
                if item['section'] == 'peticha':
                    target_dir = sections['פתיחה']
                elif item['section'] == 'introduction':
                    target_dir = sections['הקדמה']
                elif item['section'] == 'positive_mitzvot':
                    target_dir = sections['מצוות_עשה']
                elif item['section'] == 'negative_mitzvot':
                    target_dir = sections['מצוות_לא_תעשה']
                else:
                    target_dir = self.base_dir
                    
            elif item_type == 'root':
                target_dir = sections['שרשים']
                
            elif item_type == 'positive_mitzvah':
                target_dir = sections['מצוות_עשה']
                
            elif item_type == 'negative_mitzvah':
                target_dir = sections['מצוות_לא_תעשה']
                
            elif item_type == 'reference':
                if item['section'] == 'sources':
                    target_dir = sections['מקורות']
                elif item['section'] == 'external_links':
                    target_dir = sections['קישורים_חיצוניים']
                else:
                    target_dir = self.base_dir
            else:
                target_dir = self.base_dir
            
            # Move file to appropriate directory
            source_path = self.base_dir / txt_filename
            target_path = target_dir / txt_filename
            
            if source_path.exists():
                source_path.rename(target_path)
                logging.info(f"Moved {txt_filename} to {target_dir}")

    def get_existing_files(self) -> set:
        """Get set of existing file names (without extension)."""
        existing_files = set()
        if self.base_dir.exists():
            for file_path in self.base_dir.glob("*.txt"):
                existing_files.add(file_path.stem)
        return existing_files
    
    def filter_items_for_download(self) -> List[Dict]:
        """Filter items to download based on recovery mode."""
        if not self.recovery_mode:
            return self.prep_data['download_queue']
        
        # In recovery mode, skip already downloaded files
        existing_files = self.get_existing_files()
        remaining_items = []
        
        for item in self.prep_data['download_queue']:
            txt_filename = item['filename'].replace('.html', '.txt')
            if txt_filename.replace('.txt', '') not in existing_files:
                remaining_items.append(item)
        
        logging.info(f"Recovery mode: {len(existing_files)} files already exist, {len(remaining_items)} remaining to download")
        return remaining_items
    
    def download_worker(self, item: Dict) -> Tuple[bool, str]:
        """Worker function for parallel downloads."""
        url = item['url']
        filename = item['filename']
        
        success = self.download_page(url, filename)
        
        # Update progress
        with self.progress_lock:
            self.completed_items += 1
            progress = (self.completed_items / self.total_items) * 100
            logging.info(f"Progress: {self.completed_items}/{self.total_items} ({progress:.1f}%) - {filename}")
        
        return success, filename
    
    def download_all(self, delay=0.5):
        """Download all pages from the prep file using parallel processing."""
        items_to_download = self.filter_items_for_download()
        
        if not items_to_download:
            logging.info("No files to download!")
            return
        
        logging.info(f"Starting download of {len(items_to_download)} pages using {self.max_workers} workers")
        
        # Use ThreadPoolExecutor for parallel downloads
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit all download tasks
            future_to_item = {executor.submit(self.download_worker, item): item for item in items_to_download}
            
            # Process completed downloads
            for future in concurrent.futures.as_completed(future_to_item):
                try:
                    success, filename = future.result()
                    if success:
                        logging.info(f"✓ Completed: {filename}")
                    else:
                        logging.error(f"✗ Failed: {filename}")
                    
                    # Add delay between completions to be respectful to the server
                    time.sleep(delay)
                    
                except Exception as e:
                    item = future_to_item[future]
                    logging.error(f"Exception in download worker for {item['filename']}: {str(e)}")
        
        # Organize files into nested structure
        logging.info("Organizing files into directory structure...")
        self.organize_files()
        
        # Print summary
        self.print_summary()

    def print_summary(self):
        """Print download summary."""
        print("\n" + "="*50)
        print("DOWNLOAD SUMMARY")
        print("="*50)
        print(f"Total pages: {len(self.prep_data['download_queue'])}")
        print(f"Successfully downloaded: {self.downloaded_count}")
        print(f"Failed downloads: {self.failed_count}")
        print(f"Success rate: {(self.downloaded_count/len(self.prep_data['download_queue'])*100):.1f}%")
        
        if self.failed_urls:
            print("\nFailed URLs:")
            for url, filename, error in self.failed_urls:
                print(f"  {filename}: {error}")
        
        print(f"\nFiles saved in: {self.base_dir}")
        print("="*50)

    def create_index_file(self):
        """Create an index file with all downloaded content."""
        index_path = self.base_dir / "index.txt"
        
        with open(index_path, 'w', encoding='utf-8') as index_file:
            index_file.write("ספר המצוות - מפתח תוכן\n")
            index_file.write("=" * 50 + "\n\n")
            
            # Add structure information
            index_file.write("מבנה הספר:\n")
            for section in self.prep_data['structure']['main_sections']:
                index_file.write(f"  - {section}\n")
            index_file.write("\n")
            
            # Add file listing
            index_file.write("רשימת קבצים:\n")
            for item in self.prep_data['download_queue']:
                index_file.write(f"  - {item['filename']}\n")
        
        logging.info(f"Created index file: {index_path}")

def main():
    """Main function."""
    import argparse
    
    parser = argparse.ArgumentParser(description='Download Sefer HaMitzvot from Wikisource')
    parser.add_argument('--recovery', action='store_true', 
                       help='Recovery mode: skip already downloaded files')
    parser.add_argument('--workers', type=int, default=5,
                       help='Number of parallel workers (default: 5)')
    parser.add_argument('--delay', type=float, default=0.5,
                       help='Delay between requests in seconds (default: 0.5)')
    
    args = parser.parse_args()
    
    print("Sefer HaMitzvot Downloader")
    print("=" * 30)
    print(f"Recovery mode: {'Enabled' if args.recovery else 'Disabled'}")
    print(f"Parallel workers: {args.workers}")
    print(f"Request delay: {args.delay}s")
    print()
    
    # Use default prep file path
    prep_file = "sefer_hamitzvot_prep.json"
    
    # Create downloader
    try:
        downloader = SeferHaMitzvotDownloader(
            prep_file_path=prep_file,
            recovery_mode=args.recovery,
            max_workers=args.workers
        )
    except FileNotFoundError as e:
        print(f"Error: {str(e)}")
        print("\nTo fix this issue:")
        print("1. Make sure you're in the downloader directory")
        print("2. Run: python generate_prep_file.py")
        print("3. Then run this script again")
        return
    except (ValueError, RuntimeError) as e:
        print(f"Error: {str(e)}")
        print("\nThe prep file appears to be corrupted or invalid.")
        print("Please regenerate it by running: python generate_prep_file.py")
        return
    
    # Download all pages
    try:
        downloader.download_all(delay=args.delay)
        downloader.create_index_file()
        print("\nDownload completed successfully!")
        
    except KeyboardInterrupt:
        print("\nDownload interrupted by user")
        downloader.print_summary()
        
    except Exception as e:
        print(f"\nDownload failed: {str(e)}")
        downloader.print_summary()

if __name__ == "__main__":
    main() 